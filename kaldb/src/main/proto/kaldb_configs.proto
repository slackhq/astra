syntax = "proto3";

package slack.proto.kaldb;

option java_package = "com.slack.kaldb.proto.config";

// NodeRole is an enum that defines the various roles of KalDb components.
enum NodeRole {
  INDEX = 0;
  QUERY = 1;
  CACHE = 2;
  MANAGER = 3;
  RECOVERY = 4;
  PREPROCESSOR = 5;
};

// KalDb is a single binary consisting of multiple components.
// KaldbConfig is the uber config object for all of Kaldb.
// This config object controls the role a node plays and it's config.
message KaldbConfig {
  S3Config s3_config = 2;
  IndexerConfig indexer_config = 3;
  QueryServiceConfig query_config = 4;
  MetadataStoreConfig metadata_store_config = 5;
  repeated NodeRole node_roles = 6;
  TracingConfig tracing_config = 7;
  CacheConfig cache_config = 8;
  ManagerConfig manager_config = 9;
  RecoveryConfig recovery_config = 10;
  PreprocessorConfig preprocessor_config = 11;
  ClusterConfig cluster_config = 12;
}

// Static configuration for the Kaldb cluster.
message ClusterConfig {
  // A string representing name of the cluster.
  string cluster_name = 1;
  // A string representing the environment the cluster is running in.
  string env = 2;
}

// Configuration for Kafka consumer.
message KafkaConfig {
  string kafka_topic = 1;
  string kafka_topic_partition = 2;
  string kafka_boot_strap_servers = 3;
  string kafka_client_group = 4;
  string enable_kafka_auto_commit = 5;
  string kafka_auto_commit_interval = 6;
  string kafka_session_timeout = 7;
  map<string, string> additional_props = 8;
}

message MetadataStoreConfig {
  ZookeeperConfig zookeeper_config = 1;
}

// Configuration for Zookeeper metadata store.
message ZookeeperConfig {
  string zk_connect_string = 1;
  string zk_path_prefix = 2;
  int32 zk_session_timeout_ms = 3;
  int32 zk_connection_timeout_ms = 4;
  int32 sleep_between_retries_ms = 5;
}

// S3 Configuration.
message S3Config {
  string s3_access_key = 1;
  string s3_secret_key = 2;
  string s3_region = 3;
  string s3_end_point = 4;
  string s3_bucket = 5;
  double s3_target_throughput_gbps = 6;
}

message TracingConfig {
  string zipkin_endpoint = 1;
  map<string, string> common_tags = 2;
}

// Configuration for the query service.
message QueryServiceConfig {
  ServerConfig server_config = 1;
  int32 default_query_timeout_ms = 2;
  string managerConnectString = 3;
}

enum KafkaOffsetLocation {
  EARLIEST = 0;
  LATEST = 1;
}

// Configuration for the indexer.
message IndexerConfig {
  // Chunk config
  int64 max_messages_per_chunk = 1;
  int64 max_bytes_per_chunk = 2;

  // Lucene config
  LuceneConfig lucene_config = 3;

  // How long it's okay for a chunk to be on disk. If any chunks are found on
  // disk that were created longer than this many seconds ago, they will be
  // pruned (even if they are under the number of max_chunks_on_disk). This
  // check will be executed every time we roll over a chunk.
  int64 stale_duration_secs = 4;

  // Name of the data transformation pipeline to use when ingesting the data.
  string data_transformer = 5;
  // Folder where data is persisted locally on disk.
  string data_directory = 6;
  // Indexer server config.
  ServerConfig server_config = 7;
  // The max_offset_delay controls by how many kafka messages the indexer can be behind
  // before it needs to create a recovery task to catch up.
  int64 max_offset_delay_messages = 8;
  int32 default_query_timeout_ms = 9;

  KafkaConfig kafka_config = 10;

  // Determining where to read from the location when we start up
  KafkaOffsetLocation read_from_location_on_start = 11;

  // Whether or not to create recovery tasks when the indexer boots up and
  // is behind.
  bool create_recovery_tasks_on_start = 12;

  // After rolling over a chunk, we attempt to delete any chunks on disk
  // past the max_chunks_on_disk. Chunks will be deleted in an oldest
  // to newest order.
  int32 max_chunks_on_disk = 13;
}

// A config object containing all the lucene configs.
message LuceneConfig {
  int64 commit_duration_secs = 1;
  int64 refresh_duration_secs = 2;
  bool enable_full_text_search = 3;
}

// ServerConfig contains the address and port info of a Kaldb service.
message ServerConfig {
  int32 server_port = 1;
  string server_address = 2;
  int32 request_timeout_ms = 3;
}

// Configuration for cache node.
message CacheConfig {
  // Number of slots per cache instance.
  int32 slots_per_instance = 1;
  string data_directory = 2;
  // Path on local disk to store downloaded files.
  ServerConfig server_config = 3;
  int32 default_query_timeout_ms = 4;
  string replica_set = 5;
}

// Cluster manager config. As a convention we define a config struct for
// every service in the cluster manager.
message ManagerConfig {
  message ReplicaCreationServiceConfig {
    int32 schedule_period_mins = 1;
    int32 replica_lifespan_mins = 2;
    repeated string replica_sets = 3;
  }

  message ReplicaAssignmentServiceConfig {
    int32 schedule_period_mins = 1;
    repeated string replica_sets = 2;
    int32 max_concurrent_per_node = 3;
  }

  message ReplicaEvictionServiceConfig {
    int32 schedule_period_mins = 1;
  }

  message ReplicaDeletionServiceConfig {
    int32 schedule_period_mins = 1;
  }

  message RecoveryTaskAssignmentServiceConfig {
    int32 schedule_period_mins = 1;
  }

  message SnapshotDeletionServiceConfig {
    int32 schedule_period_mins = 1;
    int32 snapshot_lifespan_mins = 2;
  }

  message ReplicaRestoreServiceConfig {
    int32 schedule_period_mins = 1;
    int32 max_replicas_per_request = 2;
    int32 replica_lifespan_mins = 3;
    repeated string replica_sets = 4;
  }

  // Event aggregation secs is a de-bounce setting. It's the time
  // a service waits to take an action after a zk notification.
  int32 event_aggregation_secs = 1;
  // Initial delay is the time a cluster manager service waits
  // before the first run.
  int32 schedule_initial_delay_mins = 2;
  // Cluster manager server config.
  ServerConfig server_config = 3;

  ReplicaCreationServiceConfig replica_creation_service_config = 4;
  ReplicaAssignmentServiceConfig replica_assignment_service_config = 5;
  ReplicaEvictionServiceConfig replica_eviction_service_config = 6;
  ReplicaDeletionServiceConfig replica_deletion_service_config = 7;
  RecoveryTaskAssignmentServiceConfig
      recovery_task_assignment_service_config = 8;
  SnapshotDeletionServiceConfig snapshot_deletion_service_config = 9;
  ReplicaRestoreServiceConfig replica_restore_service_config = 10;
}

// Config for the recovery node.
message RecoveryConfig {
  ServerConfig server_config = 1;

  KafkaConfig kafka_config = 10;
}

// Config for the preprocessor node.
message PreprocessorConfig {
  // Configuration for the kafka stream processor
  message KafkaStreamConfig {
    string bootstrap_servers = 1;
    // An identifier for the stream processing application. Must be unique within the Kafka cluster
    string application_id = 2;
    // The number of threads to execute stream processing
    int32 num_stream_threads = 3;
    // This will allow parallel processing up to the amount of upstream partitions. You cannot have
    // more threads than you have upstreams due to how the work is partitioned. E
    string processing_guarantee = 4;
    // any additional kafka props that we will add when creating the KafkaStreams object
    // There are three sets of configs under this
    // 1. StreamsConfig
    // 2. ProducerConfig - These properties must be prefixed with "producer."
    // 2. ConsumerConfig - These properties must be prefixed with "consumer."
    map<string, string> additional_props = 5;
  }

  ServerConfig server_config = 1;
  KafkaStreamConfig kafka_stream_config = 2;

  // Upstream topics to consume from
  repeated string upstream_topics = 3;

  // Downstream topic to write to
  string downstream_topic = 4;

  // Name of the data transformation pipeline to use when ingesting the data.
  string data_transformer = 5;

  // The number of preprocessor instances
  // Used for calculating target throughput per instance
  int32 preprocessor_instance_count = 6;

  // Amount of time in seconds the rate limiter can burst
  int32 rate_limiter_max_burst_seconds = 7;

  // used to route all preprocessor data produced to the same kafka partition for the duration of time provided
  // value must be more than 0
  // more docs on PreprocessorPartitioner#getDatasetPartitionSuppliers
  int32 kafka_partition_sticky_timeout_ms = 8;

  // this value needs to be set if the bulk API is used to bootstrap the producer kafka
  // we plan on moving everything to the bulk API and removing KafkaStreamConfig in the future
  string bootstrap_servers = 9;
  bool use_bulk_api = 10;
}
